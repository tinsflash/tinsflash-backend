// services/chatService.js
import OpenAI from "openai";
import fetch from "node-fetch";
import { addLog } from "./logsService.js";

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

/**
 * ü§ñ Chat avec J.E.A.N. (GPT-5 ‚Üí fallback Gemini ‚Üí fallback HuggingFace)
 */
export async function chatWithJean(message) {
  // --- GPT-5 (OpenAI) ---
  try {
    addLog("‚ö° Tentative r√©ponse avec GPT-5...");
    const gpt = await openai.chat.completions.create({
      model: "gpt-5", // ton moteur m√©t√©o nucl√©aire
      messages: [
        {
          role: "system",
          content:
            "Tu es J.E.A.N., chef m√©canicien de la centrale nucl√©aire m√©t√©o mondiale. " +
            "Expert en m√©t√©orologie, climatologie et math√©matiques. " +
            "Explique toujours de mani√®re claire, fiable et pr√©cise les pr√©visions et alertes.",
        },
        { role: "user", content: message },
      ],
    });

    const reply = gpt.choices[0].message.content;
    addLog("‚úÖ R√©ponse obtenue via GPT-5");
    return { engine: "GPT-5", text: reply };
  } catch (err) {
    addLog("‚ö†Ô∏è GPT-5 indisponible: " + err.message);
    console.error("‚ùå D√©tail GPT-5:", err);
  }

  // --- Gemini (Google AI Studio) ---
  try {
    addLog("‚ö° Tentative r√©ponse avec Gemini...");
    const gemini = await fetch(
      `https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=${process.env.GEMINI_API_KEY}`,
      {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          contents: [{ role: "user", parts: [{ text: message }] }],
        }),
      }
    );

    if (!gemini.ok) {
      const errorText = await gemini.text();
      throw new Error(`Gemini API error ${gemini.status}: ${errorText}`);
    }

    const geminiData = await gemini.json();
    const reply = geminiData?.candidates?.[0]?.content?.parts?.[0]?.text;

    if (reply) {
      addLog("‚úÖ R√©ponse obtenue via Gemini");
      return { engine: "Gemini", text: reply };
    } else {
      throw new Error("R√©ponse Gemini vide ou mal form√©e");
    }
  } catch (err) {
    addLog("‚ö†Ô∏è Gemini indisponible: " + err.message);
    console.error("‚ùå D√©tail Gemini:", err);
  }

  // --- HuggingFace ---
  try {
    addLog("‚ö° Tentative r√©ponse avec HuggingFace...");
    const hf = await fetch("https://api-inference.huggingface.co/models/gpt2", {
      method: "POST",
      headers: {
        Authorization: `Bearer ${process.env.HF_API_KEY}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify({ inputs: message }),
    });

    if (!hf.ok) {
      const errorText = await hf.text();
      throw new Error(`HuggingFace API error ${hf.status}: ${errorText}`);
    }

    const hfData = await hf.json();
    const reply = hfData[0]?.generated_text;

    if (reply) {
      addLog("‚úÖ R√©ponse obtenue via HuggingFace");
      return { engine: "HuggingFace", text: reply };
    } else {
      throw new Error("R√©ponse HuggingFace vide ou mal form√©e");
    }
  } catch (err) {
    addLog("‚ö†Ô∏è HuggingFace indisponible: " + err.message);
    console.error("‚ùå D√©tail HuggingFace:", err);
  }

  // --- Fallback ultime ---
  return {
    engine: "Fallback",
    text: "‚ùå Aucune IA disponible actuellement. V√©rifiez vos cl√©s API.",
  };
}

export default { chatWithJean };
